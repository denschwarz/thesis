% !TEX root = Master_Thesis.tex
\chapter{Analysis}
\label{ch:Ana}
	This chapter will cover the analysis performed for this thesis. A basic idea of the goals and strategy of this analysis is given in section \ref{sec:strategy}. A suitable jet clustering algorithm is studied and chosen on particle level in section \ref{sec:jet_studies}. Here, also the selection on particle level is discussed. In section \ref{sec:selection}, details of the selection on reconstruction level are presented. Furthermore, the final measurement phase space including a jet energy correction for XCone jets is discussed, resulting in a jet mass distribution put into the unfolding. The unfolding procedure is presented in section \ref{sec:unfolding}.
	
\section{Analysis Strategy}
\label{sec:strategy}
	This analysis aims for boosted $t\bar{t}$ where all decay products from the top decays merge into a single jet. For this measurement the lepton + jets channel is used, where the event is tagged with the lepton, suppressing backgrounds. The measurement of the jet mass is then performed with the hadronically decaying top quark. The detailed selection is presented in section \ref{sec:selection}. In the required phase space the distribution of the jet mass reconstructing a top quark decaying into quarks ($t\rightarrow b q \bar{q}'$) is measured. Following an unfolding is performed using the TUnfold \cite{tunfold} software package. The goal is to compare data unfolded to particle level with first principle calculations. Studies on particle level are presented in section \ref{sec:jet_studies} to find a jet algorithm that fits this purpose.
	

\section{Jet Studies on Particle Level}
\label{sec:jet_studies}
	For this analysis, it is crucial to choose a suitable jet algorithm and cone size. The previously mentioned analysis at $8\;\text{TeV}$ \cite{torben_paper} uses Cambridge-Aachen jets with the radius parameter set to $R=1.2$ for its measurement. This large radius was chosen to compensate for low statistics in the boosted $t\bar{t}$ regime. Since the cross-section of $t\bar{t}$ production is much higher at a center-of-mass energy of $13\;\text{TeV}$ a smaller cone is expected to be applicable for this analysis. Additionally jets clustered with Anti-$k_T$, HOTVR and XCone algorithms are studied and optimized in sections \ref{sec:AK}, \ref{sec:HOTVR2} as well as \ref{sec:XCone_strat} and finally compared in section \ref{sec:jet_comp} to find the algorithm most suitable for this analysis. The goal is to select a jet algorithm which returns jets in which all decay products of a hadronically decaying top quark are merged. In this case, the jet mass $M_\text{jet}$ is sensitive to the top quark mass $M_\text{top}$. The jet mass distribution should return a sharp peak at the top quark mass of around $173\;\text{GeV}$ to be able to extract the top quark mass. Jet studies are performed with a $t\bar{t}$ simulation using the information of MC simulations at particle level. The detailed selection is described in the following.

\subsection{Selection on Particle level}
\label{sec:GenSel}
	All studies on particle level are performed with a $t\bar{t}$ sample only. Several selection criteria are used to select boosted top quark decays in the lepton + jets channel. Since the selection is applied on particle level, also particle level information is used. The selection reads:
	\begin{itemize}
	\item direct selection of lepton + jets channel,
	\item exactly one electron or muon,
	\item veto on additional leptons,
	\item $p_T^{\text{1st jet}} > 400\;\text{GeV}$,
	\item $p_T^{\text{2nd jet}} > 200\;\text{GeV}$,
	\item Veto on additional jets with $p_T > 200\;\text{GeV}$,
	\item $\Delta R (\text{lepton, 2nd jet}) < \text{jet radius}$ and
	\item $M^{\text{1st jet}} > M^{\text{2nd jet + lepton}}$.
	\end{itemize}
	Where the first jet refers to the leading jet in $p_T$ of the respective jet clustering algorithm. It is expected to be originating from the hadronically decaying top quark, the second one is expected to contain the products of the leptonically decaying top quark. The purpose of the $p_T$ thresholds is to select boosted top decays. The veto on additional jets is set to select $t\bar{t}$ events where one jet per top quark decay is expected. It is to mention, that the veto is not present in combination with XCone since it will always return exactly two jets in the used set up. A cut on the distance between lepton and second leading jet in $p_T$ prefers boosted topologies where the lepton is inside the jet of the leptonically decaying top quark. This criterion is also obsolete for XCone because of the selected clustering sequence (see section \ref{sec:XCone_strat}). To suppress events where not all decay products of the hadronically decaying top quark end up in the jet with the highest transverse momentum, a mass criterion $M^{\text{1st jet}} > M^{\text{2nd jet + lepton}}$ is set. The criterion includes the assumption that the mass of the jet on the leptonic side is lower because the neutrino cannot be reconstructed. This selection is applied to every jet algorithm output to be able to compare these different approaches. All shown distributions are scaled to match a integrated luminosity of $37.76\;\text{fb}^{-1}$.

\FloatBarrier %draw figures of previous section before the new one starts	
\subsection{Studies with Anti-$k_T$}
\label{sec:AK}	
	Since the top quark decay should be reconstructed with one jet, all decay products need to end up inside the defined jet cone. Choosing different cone sizes has various effects. When the cone is small, not all decay products may end up in the jet and the jet mass is reconstructed smaller than the top mass. If the cone size is large, the probability to include additional radiation and pile-up grows and the resulting jet mass is reconstructed too high.	As a starting point Anti-$k_T$ jets with a radius of $0.8$ are selected since this is the CMS intern standard to reconstruct top quark jets.  AK jets with a radius of $0.8$ and $1.2$ are presented in Fig. \ref{fig:GEN_AK08} and \ref{fig:GEN_AK12}, respectively, to study the influence of the cone size. Additionally, a matching to generated particles is performed. If all three decay products of the top quark are clustered into the jet, the jet is called 'matched'. One can see that AK8 jets tend to deliver a jet mass lower than the top quark mass. This is due to the higher fraction of 'not matched' events. In this case, these are events where not every decay product ends up in the jet. AK12 jets on the other hand often return a mass higher than the top quark mass which is due to underlying event effects. A larger cone size has a higher probability of including particles not originating from the top quark one is interested in. Furthermore, even the peak position for AK12 jets is reconstructed above the top quark mass. It is also worth to mention that with a larger cone size more events survive the selection criteria because a large jet sums up more particles and thus more transverse momentum. Because of the better resolution in the peak region and much less tail, AK8 fits the purpose of this analysis well and is further used. To reduce additional energy clustered into the jet further, grooming algorithms like soft drop are used. A comparison of AK8 jets with and without soft drop is depicted in Fig. \ref{fig:GEN_AK08sd}. With soft drop applied, the $W$ peak at $80\;\text{GeV}$ can be clearly identified. Additionally, masses above the top quark mass are reduced. The distribution with AK8 jets and soft drop (Fig. \ref{fig:GEN_AK08sd1}) will be compared to other clustering methods in section \ref{sec:jet_comp}.

	\begin{figure}[tb]
		\begin{subfigure}{.5\textwidth}
	    \centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/AK08_matching}
		\caption{}
		\label{fig:GEN_AK08}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/AK12_matching}
		\caption{}
		\label{fig:GEN_AK12}
		\end{subfigure}
		\caption{Comparison of jet mass distributions of AK8 (a) and AK12 (b) jets. A smaller cone size (a) leads to a lower reconstructed mass while a large cone (b) returns higher masses. The fraction of 'matched' and 'not matched' events is shown in the histograms.}
	\end{figure}
	
	\begin{figure}[tb]
		\begin{subfigure}{.5\textwidth}
	    \centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/AK08softdrop_matching}
		\caption{}
		\label{fig:GEN_AK08sd1}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/AK08_matching}
		\caption{}
		\label{fig:GEN_AK08sd2}
		\end{subfigure}
		\caption{Comparison of jet mass distributions of AK8 with (a) and without (b) the soft drop algorithm applied. The influence of the grooming is especially visible between the $W$ boson mass and the top quark mass.}
		\label{fig:GEN_AK08sd}
	\end{figure}
	
\FloatBarrier %draw figures of previous section before the new one starts			
\subsection{Studies with HOTVR}
\label{sec:HOTVR2}
	Another approach to find an appropriate cone size is to use not a constant but $p_T$ dependent radius parameter. Since decay products are Lorentz boosted with high momentum, the higher the $p_T$, the smaller cone size is necessary to contain all decay products. Thus, HOTVR (see section \ref{sec:HOTVR}) directly addresses this property. The default settings set the effective radius to $R_\text{eff} = \frac{600\;\text{GeV}}{p_T}$, which corresponds to a maximum radius of $1.5$ with the used selection of jets with $p_T > 400\;\text{GeV}$. The result is visible in Fig. \ref{fig:GEN_HOTVR}. Due to the large cone size for the lowest possible jet momentum, the distribution is very similar to a AK jet with a large radius parameter. The parameter to tune the HOTVR clustering is $\rho$. By lowering $\rho$, the effective radius shrinks. Figure \ref{fig:GEN_HOTVRrho} shows a comparison of the jet mass for different $\rho$. It is decreased in $100\;\text{GeV}$ steps to a value of $300\;\text{GeV}$ which corresponds to a radius of $0.75$ for jets with a transverse momentum of $400\;\text{GeV}$. A behaviour similar to decreasing the radius of Anti-$k_T$ jets is visible. Increasing $\rho$ returns more events where all decay products end up in the jet cone, but a jet also includes more additional particles leading to a higher mass. Since the setting $\rho = 400\;\text{GeV}$ returns the highest fraction of events with a mass around the top quark mass, it will act as representative for HOTVR in the comparison presented in section \ref{sec:jet_comp}.

	\begin{figure}[h]
		\centering
		\includegraphics [width=.6\textwidth]{../Plots/GenStudies/HOTVR_matching}
		\caption{Jet mass distribution of jets clustered with the HOTVR algorithm. Here, default values of the clustering procedure are used. The large tail can be explained with large jet radii for jets with a transverse momentum around $400\;\text{GeV}$.}
		\label{fig:GEN_HOTVR}
	\end{figure}	
	
	\begin{figure}[tb]
		\begin{subfigure}{.5\textwidth}
	    \centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/HOTVRrho600_matching}
		\caption{}
		\label{fig:GEN_HOTVR600}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/HOTVRrho500_matching}
		\caption{}
		\label{fig:GEN_HOTVR500}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
	    \centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/HOTVRrho400_matching}
		\caption{}
		\label{fig:GEN_HOTVR400}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/HOTVRrho300_matching}
		\caption{}
		\label{fig:GEN_HOTVR300}
		\end{subfigure}		
		\caption{Study of the influence of the HOTVR $\rho$ parameter on the jet mass. While $\rho$ is decreasing from (a) to (d) in $100\;\text{GeV}$ steps, the effective cone radius $R_\text{eff}$ increases accordingly.}
		\label{fig:GEN_HOTVRrho}
	\end{figure}

\FloatBarrier %draw figures of previous section before the new one starts		
\subsection{Studies with XCone}
\label{sec:XCone_strat}
	The XCone jet algorithm described in section \ref{sec:xcone} has already been tested resolving $t\bar{t}$ decays. Studies for hadronically decaying top quark pairs are presented in a paper from Thaler and Wilkason \cite{xconetop}. Here, the XCone algorithm is tuned to the $t\bar{t}$ final state, expecting six jets. Using the information that it is expected to find three jets from each top quark, a promising approach to reconstruct the top quark decays was made with a strategy using two clustering steps (see Fig. \ref{fig:JetDisplay}). Firstly, XCone is required to find exactly two jets with a large radius ensuring that all decay products of the top quark end up in the jet. Thus, every particle from the hard scattering should be clustered into one of the jets. The goal of this first step is to separate the two top quarks into independent jets.  Now, the jets are identified if they contain the decay products of the hadronically decaying or leptonically decaying top quark via a distance measure to the lepton $\Delta R (\text{lepton, jet})$. To check if the categorization works, the distance $\Delta R$ between the hadronically decaying top quark on particle level and the selected jet is calculated. The distribution is shown in Fig. \ref{fig:XCone_dR}. According to this plot, almost every jet is categorized correctly. The jet shape in the $\eta$-$\phi$-plane, identification (orange indicates the jet identified as originating from the hadronically decaying top quark) and all particles in the event are shown in Fig. \ref{fig:JetDisplay1}. After that, the jets are further divided into smaller subjets. Since one expects only two visible components on the leptonic side, only two subjets are required, while the other jet contains three. This strategy will be referred to as '$2+5$' and is depicted in Fig. \ref{fig:JetDisplay2}. The subjets are then combined to form a final jet that is used from now on.	
	\begin{figure}[tb]
		\centering
		\includegraphics [width=.5\textwidth]{../Plots/GenStudies/XCone_dR_GEN_R20}
		\caption{Check of categorisation of XCone jets. The distance between generated hadronically decaying top quark and jet identified as containing its decay products is shown. The distribution is expected to peak at low values if the categorisation works, which is the case.}
		\label{fig:XCone_dR}
	\end{figure} 
	\begin{figure}[tb]
		\begin{subfigure}{.5\textwidth}
	    \centering
		\includegraphics [width=\textwidth]{../Plots/JetDisplayR15/xcone_incjets_event04}
		\caption{}
		\label{fig:JetDisplay1}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
	    \centering
		\includegraphics [width=\textwidth]{../Plots/JetDisplayR15/xcone_subjets_event04}
		\caption{}
		\label{fig:JetDisplay2}
		\end{subfigure}
		\caption{Display of the jet area from XCone jets clustered with the '$2+5$' approach after the first (a) and the second step (b) . The grey dots show particles identified by the PF algorithm, red circles indicate decay products from the hadronically decaying top quark where the bottom quark is additionally marked. Decay products from the leptonically decaying top quark are marked as follows: the black circle with star illustrates the bottom quark, the black circle with plus sign shows the lepton and the white circle marks the neutrino.}
		\label{fig:JetDisplay}
	\end{figure}
	Finally, the cone sizes have to be chosen. The subjets are set to a radius of $R=0.4$ to be comparable to the CMS standard for subjets, which are AK4 jets. Studies of the jet mass are made to determine the most suitable radius for the first clustering step. A comparison of the jet area in Fig. \ref{fig:JetDisplayR} shows a higher expected misidentification rate (subjet at the bottom in Fig. \ref{fig:JetDisplayR2}) for larger cones because of additional radiation or underlying event. This would lead to high masses especially when yet all decay products end up in the other two subjets (as in Fig. \ref{fig:JetDisplayR2}). On the other hand, a small cone may not include all decay products. This effect can also be seen in the jet mass distributions in Fig. \ref{fig:XConeR1}. Based on these studies, the radius parameter is chosen to be $R=1.2$ because of less events in the shoulder around the $W$ mass and a smaller tail.\\
	To perform the clustering with data, an easier method is tested, where both fat jets contain three subjets, referred to as '$2+6$'. Afterwards the final jets are analogously categorized into a jet originating from the hadronically and the leptonically decaying top quark. A comparison between the two methods (see Fig \ref{fig:GEN_XCone_comp}) shows that both return almost the same distribution. Thus, the '$2+6$' method is chosen to represent the XCone result.
	
	\begin{figure}[tb]
		\begin{subfigure}{.5\textwidth}
	    \centering
		\includegraphics [width=\textwidth]{../Plots/JetDisplayR10/xcone_subjets_event09}
		\caption{}
		\label{fig:JetDisplayR1}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
	    \centering
		\includegraphics [width=\textwidth]{../Plots/JetDisplayR20/xcone_subjets_event09}
		\caption{}
		\label{fig:JetDisplayR2}
		\end{subfigure}
		\caption{Jet area comparison between a small (a) and a large $R_1$ (b). The small cone only barely contains all decay products while the larger cone leads to misidentification of particles not belonging to the top quark decay.}
		\label{fig:JetDisplayR}
	\end{figure}	
		
	\begin{figure}[tb]
		\begin{subfigure}{.5\textwidth}
  		\centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/XCone_GEN_R10}
		\caption{}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
  		\centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/XCone_GEN_R12}
		\caption{}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
  		\centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/XCone_GEN_R15}
		\caption{}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
  		\centering
		\includegraphics [width=\textwidth]{../Plots/GenStudies/XCone_GEN_R20}
		\caption{}
		\end{subfigure}
						
		\caption{Jet mass distributions for different $R_1$. The smaller $R_1$ is, the more jets are reconstructed at the $W$ mass. If $R_1$ increases, the probability of radiation ending up in the final jet grows and jets are more likely to have a mass above the top quark mass visible in the tail above $200\;\text{GeV}$.}
		\label{fig:XConeR1}
	\end{figure}	
	
 	\begin{figure}[tb]
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth]{../Plots/GenStudies/XCone23_matching}
 		\label{fig:GEN_XCone23}
 		\caption{}
 		\end{subfigure}
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth]{../Plots/GenStudies/XCone33_matching}
 		\label{fig:GEN_XCone33}
 		\caption{}
 		\end{subfigure}
 		\caption{Comparison of the jet mass distribution of XCone jets clustered with the '$2+5$' (a) and the '$2+6$' (b) method. Both graphs show a very similar shape and are therefore interchangeable.}
 		\label{fig:GEN_XCone_comp}
 	\end{figure}
 	
\FloatBarrier %draw figures of previous section before the new one starts 	
\subsection{Comparing Jet Algorithms}
\label{sec:jet_comp}
	In this section, the resulting jet mass distributions of different jet clustering algorithms discussed above are compared. Figure \ref{fig:Jet_Comp} shows all jet mass distributions examined above. All three clustering methods return a good resolution in the peak region while the maximum sits at the top quark mass. The Anti-$k_T$ algorithm with a cone radius of $R=0.8$ and soft drop applied reconstructs many events with a low mass, even showing a peak at the $W$ boson mass. This effect is explained by the small cone size. If only the decay products of the $W$ boson, but not the $b$ quark is clustered, a mass around the $W$ boson mass is expected. With a variable sized cone in HOTVR, this effect is reduced but more events with a mass larger than $200\;\text{GeV}$ are observed. XCone on the other hand combines the good aspects of Anti-$k_T$ and HOTVR. It returns a jet mass distribution with a narrow peak at the top quark mass and a low fraction of masses reconstructed too low or high. Furthermore, much more jets are reconstructed correctly, therefore survive the selection criteria and lead to much higher event yield. This is explained by a softer selection of jets since an exclusive jet clustering algorithm will always return exactly two jets and a veto on additional jets is redundant. Because of the advantages of XCone, namely good resolution, peak position sensitive to the top quark mass and very high statistics, the measurement will be performed using XCone jets clustered with the '$2+6$' method.
	
 	\begin{figure}[tb]
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth]{../Plots/GenStudies/AK08softdrop_matching}
 		\label{fig:Jet_Comp_ak}
 		\caption{}
 		\end{subfigure}
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth]{../Plots/GenStudies/HOTVRrho400_matching}
 		\label{fig:Jet_Comp_HOTVR}
 		\caption{}
 		\end{subfigure}
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth]{../Plots/GenStudies/XCone33_matching}
 		\label{fig:Jet_Comp_XCone}
 		\caption{}
 		\end{subfigure} 		
 		\caption{Jet mass distributions of jets clustered with Anti-$k_T$ (a), HOTVR (b) and XCone (c). While all distributions show a good resolution in the peak region, AK shows a large fraction of masses reconstructed too low and XCone returns the largest statistics.}
 		\label{fig:Jet_Comp}
 	\end{figure}	
	
\section{Studies on Reconstruction Level}
\label{sec:selection}
	Since the measurement should be performed with data, a similar jet mass distribution has to be achieved on reconstruction level. A selection is applied to simulation and data to obtain a dataset consisting of mostly $t\bar{t}$ events in the lepton+jets channel. The selection can be divided into two steps. Firstly, a baseline selection is used to suppress background processes (see section \ref{sec:PreSel}). Secondly, the final phase space is defined (see section \ref{sec:FinalSel}) to select $t\bar{t}$ events with boosted top quarks. This is crucial for this analysis because the goal is to reconstruct the top quark with one jet. This can only be done if all of its decay products merge into one jet.

\subsection{Baseline Selection}
\label{sec:PreSel}
	In the lepton+jets channel of the $t\bar{t}$ process one expects to find exactly one muon or electron, two small jets from the hadronically decaying $W$ boson, two b-jets and missing transverse energy since the neutrino cannot be detected. This baseline selection is designed to remove non-$t\bar{t}$ events. Since this analysis focuses on the muon channel, the selection reads:
	\begin{itemize}
	\item single muon trigger (combination of "HLT\_Mu50\_v*" and "HLT\_TkMu50\_v*") with $p_T > 50\;\text{GeV}$ threshold,
	\item exactly one tight muon with $p_T > 55\;\text{GeV}$ and $|\eta| < 2.4$,
	\item veto on additional leptons,
	\item two-dimensional muon isolation criterion: \\ $\Delta R(\text{lepton, next AK4 jet}) > 0.4$ or $p_T^{\text{rel}}(\text{lepton, next AK4 jet}) > 40\;\text{GeV}$ \footnote{$p_T^{\text{rel}}(a,b) = \frac{|\vec{p_a} \times \vec{p_b}|}{|\vec{p_b}|}$},
	\item at least two AK4 jets with $p_T > 50\;\text{GeV}$ and $|\eta| < 2.4$,
	\item $\cancel{E}_T > 50\;\text{GeV}$ and
	\item at least one tight b-tag.

	\end{itemize}
	Because the dataset corresponding to the called single muon trigger is used in this analysis, the trigger criteria have to be fulfilled in simulation as well. An additional cut on the muon $p_T$ above $53\;\text{GeV}$ is recommended \cite{MuonID} for this trigger to reach the plateau of trigger efficiency. Furthermore, a scale factor is applied to simulation to account for efficiency differences in data and simulation. Since only one lepton is expected in the lepton+jets channel of $t\bar{t}$, a veto on additional leptons is used to suppresses diboson events. Because no isolation criteria is checked for the muons a two-dimensional cut is applied to reject QCD events. A window in the $\Delta R$-$p_T^{\text{rel}}$-plane is cut out where the majority of QCD that survive the lepton criteria accumulates. A display of the two-dimensional cut is presented in Fig. \ref{fig:2D}. The requirements to find two AK4 jets with at least $50\;\text{GeV}$, missing transverse energy of at least $50\;\text{GeV}$ and a b-tag prefer $t\bar{t}$ events because of the similar signature. For b-tagging a scale factor is applied to match efficiency in data and simulation. After applying the selection the remaining events contain about $80\%$ $t\bar{t}$, the main remaining backgrounds are $W+$jets and Single-Top production. 
 	\begin{figure}[tb]
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth]{../Plots/TwoD_QCD}
 		\caption{}
 		\end{subfigure}
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth]{../Plots/TwoD_TTbar}
 		\caption{}
 		\end{subfigure}
 		\caption{Distribution in the $\Delta R$-$p_T^{\text{rel}}$-plane for QCD (a) and $t\bar{t}$ events (b). The window affected by the 2D cut is surrounded by red lines. While QCD events mostly accumulate in the low left corner, many $t\bar{t}$ events will survive this cut.}
 		\label{fig:2D}
 	\end{figure}
 	After applying the baseline selection, a difference in total number of events between simulation and data  as well as a $p_T$ dependent trend is visible (see Fig. \ref{fig:PreSeljet}), probably coming from a mismodelled top quark $p_T$ distribution. The difference in $p_T$ spectra of the top quark has been observed in several publications, for example in a $t\bar{t}$ differential cross section measurement from CMS \cite{ttreweight}. An observed ratio between data and simulation in shown in Fig. \ref{fig:topreweight}. To justify this assumption a re-weighting of the top quark $p_T$ spectrum in $t\bar{t}$ simulation is applied according to Fig. \ref{fig:topreweight}. The resulting histograms are presented in Fig. \ref{fig:PreSel_reweight}. After this procedure, MC and data are well in agreement except for the AK4 jet $p_T$ in high energy regions. This is expected to not be well corrected by the re-weighting since it covers the range from $400\;\text{GeV}$ upwards with only one constant value. This effect is also visible in the measurement phase space discussed below and is one of the simulation properties that could be validated with this analysis.
 	 		
 	\begin{figure}[tb]
  		\centering
 		\includegraphics [width=.65\textwidth, trim=0 0 0 0, clip]{../Plots/../Plots/PreSel/08_bTag_jets/pt_jet_log.pdf}
 		\caption{Transverse momenta of all AK4 jets in the event. A trend in the ratio between data and simulation is observed.}
 		\label{fig:PreSeljet}
 	\end{figure}
 	
  	\begin{figure}[tb]
   		\centering
  		\includegraphics [width=.5\textwidth]{../Plots/top_reweight}
  		\caption{Ratio between data and simulation in differential top quark pair cross section in dependence on the transverse momentum of top quarks. Taken from \cite{topreweight}.}
  		\label{fig:topreweight}
  	\end{figure}	
 	
 	\begin{figure}[tb]
 		\begin{subfigure}{.5\textwidth}
  		\centering
		\includegraphics [width=\textwidth, trim=0 0 0 0, clip]{../Plots/PreSel/ttbar_reweight_Muon/number_lin.pdf}
 		\caption{}
 		\end{subfigure}
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth, trim=0 0 0 0, clip]{../Plots/PreSel/ttbar_reweight_Muon/pt_1_log.pdf}
 		\caption{}
 		\end{subfigure} 		
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth, trim=0 0 0 0, clip]{../Plots/PreSel/ttbar_reweight_Jets/number_lin.pdf}
 		\caption{}
 		\end{subfigure}
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth, trim=0 0 0 0, clip]{../Plots/PreSel/ttbar_reweight_Jets/pt_jet_log.pdf}
 		\caption{}
 		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth, trim=0 0 0 0, clip]{../Plots/PreSel/ttbar_reweight_Event/BTAG_T_lin.pdf}
 		\caption{}
 		\end{subfigure} 	
  		\begin{subfigure}{.5\textwidth}
   		\centering
  		\includegraphics [width=\textwidth, trim=0 0 0 0, clip]{../Plots/PreSel/ttbar_reweight_Event/MET_log.pdf}
  		\caption{}
  		\end{subfigure}  	 			
 		\caption{Control distributions after applying a reweighting of the top quark $p_T$ in $t\bar{t}$ simulation. Displayed are number of muons (a), $p_T$ spectrum of muons (b), number of AK4 jets (c), $p_T$ distribution of all AK4 jets (d), number of b-tagged AK4 jets (e) and the spectrum of the missing transverse energy (f). }
 		\label{fig:PreSel_reweight}
 	\end{figure}	

\FloatBarrier %draw figures of previous section before the new one starts
\subsection{Measurement Phase Space}
\label{sec:FinalSel}
	The measurement phase space on reconstruction level is defined analogously to the particle level selection. Boosted topologies are selected by requiring the leading jet to surpass a cut on $p_T > 400\;\text{GeV}$. In addition, the mass of the leading jet is expected to be higher then the second jet mass if all top quark decay products are reconstructed correctly. Thus, on top of the baseline selection presented above, following criteria are checked:
	\begin{itemize}
	\item only sum subjets with $p_T > 30\;\text{GeV}$,
	\item $p_T^{\text{1st jet}} > 400\;\text{GeV}$ and
	\item $M^{\text{1st jet}} > M^{\text{2nd jet + lepton}}$.
	\end{itemize}
	In this phase space, the selection criteria refer to XCone jets clustered with the '$2+6$' method.	In addition to the definition on particle level, only subjets with a $p_T$ larger then $30\;\text{GeV}$ are considered to form the final jet. This requirement is set so suppress subjets which do not contain any decay product of the top quark. Figure \ref{fig:MJet_raw1} shows the jet mass distribution after the called jet requirements. Similar as on particle level, XCone returns a peak at the expected bin of the top quark mass. Thus, the reconstruction with XCone jets works very well. As seen after the baseline selection, simulation exceeds the number of events in data. In Fig. \ref{fig:MJet_raw2}, $t\bar{t}$ simulation is scaled with a constant factor to match the total events in data and simulation. A good agreement between data and MC is visible. Of course, further procedure like unfolding will use the unscaled version as input. In addition, another variant of jet clustering with XCone is presented in Fig. \ref{fig:MJet_raw3}. Here, the large XCone jet with radius $R=1.2$ is put into the soft drop algorithm. Albeit the grooming of soft drop, the '2+6' method returns a much sharper mass peak. While the turn on of the soft drop mass is comparable with '2+6', there are many event with a mass reconstructed too high.
 	\begin{figure}[tb]
 		\begin{subfigure}{.5\textwidth}
  		\centering
 		\includegraphics [width=\textwidth, trim=0 0 3cm 0, clip]{../Plots/PostSel/XCone_raw/M_jet1__lin.pdf}
 		\caption{}
 		\label{fig:MJet_raw1}
 		\end{subfigure}
 		\begin{subfigure}{.5\textwidth}
 		\centering
		\begin{tikzpicture}
		 \node[anchor=south west,inner sep=0] (image) at (0,0)
		 {\includegraphics[width=\textwidth, trim=0 0 3cm 0, clip]{../Plots/PostSel/XCone_raw_SF/M_jet1__lin.pdf}};
		 \node[align=left,font=\tiny] at (2.2, 4.2) {$t\bar{t}$ scaled};
		\end{tikzpicture} 
 		\caption{}
 		\label{fig:MJet_raw2}
 		\end{subfigure}
 		\begin{subfigure}{.5\textwidth}
 		\centering
		\begin{tikzpicture}
		 \node[anchor=south west,inner sep=0] (image) at (0,0)
		 {\includegraphics[width=\textwidth, trim=0 0 3cm 0, clip]{../Plots/PostSel/XCone_raw_SF/SoftdropMass_had_lin.pdf}};
		 \node[align=left,font=\tiny] at (2.2, 4.2) {$t\bar{t}$ scaled};
		\end{tikzpicture} 
 		\caption{}
 		\label{fig:MJet_raw3}
 		\end{subfigure} 		
 		\caption{Jet mass distribution of XCone jets after applying the measurement phase space requirements. Additional to the raw output (a) a distribution where $t\bar{t}$ simulation is scaled to data is presented (b). Histogram (c) shows the soft drop mass of the large XCone jet ($R=1.2$) in comparison. It shows that a grooming via XCone subjet finding is much more effective than soft drop in this particular case.}
 		\label{fig:MJet_raw}
 	\end{figure}
	
\FloatBarrier %draw figures of previous section before the new one starts	
\subsection{Jet Energy Corrections for XCone Jets} 
	The normal procedure in CMS analyses is to apply jet energy corrections (see section \ref{sec:jec}) to every jet collection used. Those jet energy corrections (JEC) have been derived by dedicated CMS groups and are dependent on the jet algorithm used to cluster jets. Since the XCone algorithm is not a standard jet finding procedure in CMS, there are no valid corrections available. Because the final jet measured in this a analysis is a combination of subjets, only the subjets are corrected. A XCone jet with applied jet energy corrections refers then to a jet put together from corrected subjets. The first attempt to correct XCone jets is to use the AK4 jet corrections since the jet shape should be very similar to XCone subjets with $R=0.4$ as they were used in this analysis. Figure \ref{fig:MJet_jec} shows a comparison between jets with and without JEC applied. 
 	\begin{figure}[tb]
 		\begin{subfigure}{.5\textwidth}
 		\centering
		\begin{tikzpicture}
		 \node[anchor=south west,inner sep=0] (image) at (0,0)
		 {\includegraphics[width=\textwidth, trim=0 0 3cm 0, clip]{../Plots/PostSel/XCone_raw_SF/M_jet1__lin.pdf}};
		 \node[align=left,font=\tiny] at (2.2, 4.2) {$t\bar{t}$ scaled};
		\end{tikzpicture} 
 		\caption{}
 		\label{fig:MJet_jec1}
 		\end{subfigure}
 		\begin{subfigure}{.5\textwidth}
 		\centering
		\begin{tikzpicture}
		 \node[anchor=south west,inner sep=0] (image) at (0,0)
		 {\includegraphics[width=\textwidth, trim=0 0 3cm 0, clip]{../Plots/PostSel/XCone_jec_SF/M_jet1__lin.pdf}};
		 \node[align=left,font=\tiny] at (2.2, 4.2) {$t\bar{t}$ scaled};
		\end{tikzpicture} 
 		\caption{}
 		\label{fig:MJet_jec2}
 		\end{subfigure}
 		\caption{Jet mass distribution of XCone jets before (a) and after applying AK4 jet energy corrections (b). A shift in jet masses to higher values is visible.}
 		\label{fig:MJet_jec}
 	\end{figure}	
 	An obvious shift in jet mass is visible, resulting in a peak position above the top quark mass. Since the data-simulation agreement is well in both cases, different measures have to be defined to validate the jet energy correction. Now, an attempt is made to compare the subjets with a well known parameter in the theory. Therefore, all three jet mass combinations $M_{ij}$ of two of the three subjets on the hadronic side are calculated. It is expected that the combination with the lowest jet mass should be sensitive to the $W$ mass which is very precisely measured to $80.4\;\text{GeV}$ \cite{Wmass}. As shown in Fig. \ref{fig:Wmass} the $W$ boson mass is reconstructed too high at values above $85\;\text{GeV}$. Therefore, applying jet energy correction from AK4 jets to XCone subjets is not valid.
  	\begin{figure}[tb]
  		\begin{subfigure}{.5\textwidth}
  		\centering
 		\begin{tikzpicture}
 		 \node[anchor=south west,inner sep=0] (image) at (0,0)
 		 {\includegraphics[width=\textwidth, trim=0 0 3cm 0, clip]{../Plots/PostSel/XCone_raw_subjets_SF/min_mass_Wjet_zoom_lin.pdf}};
 		 \node[align=left,font=\tiny] at (2.2, 4.2) {$t\bar{t}$ scaled};
 		\end{tikzpicture} 
  		\caption{}
  		\label{fig:Wmass1}
  		\end{subfigure}
  		\begin{subfigure}{.5\textwidth}
  		\centering
 		\begin{tikzpicture}
 		 \node[anchor=south west,inner sep=0] (image) at (0,0)
 		 {\includegraphics[width=\textwidth, trim=0 0 3cm 0, clip]{../Plots/PostSel/XCone_jec_subjets_SF/min_mass_Wjet_zoom_lin.pdf}};
 		 \node[align=left,font=\tiny] at (2.2, 4.2) {$t\bar{t}$ scaled};
 		\end{tikzpicture} 
  		\caption{}
  		\label{fig:Wmass2}
  		\end{subfigure}
  		\caption{Lowest jet mass combination $\min(M_{ij})$ without (a) and with (b) JEC applied. The position of the peak is expected to match the $W$ boson mass of $80.4\;\text{GeV}$. The shift after applying JEC puts $\min(M_{ij})$ to higher values.} 
  		\label{fig:Wmass}
  	\end{figure}	
	To still be able to correct XCone jets for response non linearities and pile-up effects, a correction factor on top of AK4 corrections is derived for XCone jets. For this, only events from $t\bar{t}$ simulation are used. Furthermore, only the subjets from the jet belonging to the hadronically decaying top quark are considered. A matching to generator jets is executed and the fraction $R=\frac{p_T^{\text{rec}}}{p_T^{\text{gen}}}$ calculated. This is done in different $p_T$ and $\eta$ regions. The mean $R$ in region is then filled in a two dimensional histogram representing the $p_T$-$\eta$-plane (see Fig. \ref{fig:Correction}). The bin boundaries are chosen to obtain enough statistics in each bin to suppress uncertainties (RMS\footnote{root mean square} in the appendix in Fig. \ref{fig:A_rms}). 
		\begin{figure}[tb]
			\centering
			\includegraphics [width=.7\textwidth]{../Plots/Correction/Mean_numbers}
			\caption{Mean values of $R=\frac{p_T^{\text{rec}}}{p_T^{\text{gen}}}$ in the $p_T$-$\eta$ plane.}
			\label{fig:Correction}
		\end{figure}	
	The correction factor applied to every XCone jet is now $f = \frac{1}{R}$. In every $\eta$ bin a polynomial function is fitted to get a factor $f(p_T)$ to get a smooth transition between the different regions. An example of the fit is shown in Fig. \ref{fig:Correction_fit} (all fit functions can be found in the appendix in Fig. \ref{fig:A_fits}).
	\begin{figure}[tb]
		\centering
		\includegraphics [width=.5\textwidth]{../Plots/Correction/Fits_example}
		\caption{Example of fit function. The correction factors are derived from Fig. \ref{fig:Correction} and then fitted with a polynomial function of degree $2$.}
		\label{fig:Correction_fit}
	\end{figure}
	Now every subjet is corrected with a different $p_T$ dependent function according to its direction in $\eta$. It is to mention that because of the loose ends of the fit, the correction factor is set constant for $p_T > 425\;\text{GeV}$. To verify this correction, the minimum jet mass from two subjets is again compared with the $W$ boson mass in Fig. \ref{fig:Wmass_cor}. It is shown that the peak position is in agreement with the measured $M_W = 80.4\;\text{GeV}$.
  	\begin{figure}[tb]
  		\centering
 		\begin{tikzpicture}
 		 \node[anchor=south west,inner sep=0] (image) at (0,0)
 		 {\includegraphics[width=.7\textwidth, trim=0 0 3cm 0, clip]{../Plots/PostSel/XCone_cor_subjets_SF/min_mass_Wjet_zoom_lin.pdf}};
 		 \node[align=left,font=\small] at (3.3, 6.5) {$t\bar{t}$ scaled};
 		\end{tikzpicture} 
   		\caption{Lowest jet mass combination $\min(M_{ij})$ after AK4 jet energy correction and the correction for XCone. The peak position is in agreement with the value of the $W$ boson mass of $80.4\;\text{GeV}$.} 
  		\label{fig:Wmass_cor}
  	\end{figure}	  
  	Furthermore, a matching via $\Delta R$ between jets on particle and reconstruction level is performed to calculate the relative $p_T$ deviation. A graph showing the mean and width (here calculated as RMS) of $\frac{p_T^\text{rec} - p_T^\text{gen}}{p_T^\text{gen}}$ in bins of $p_T^\text{rec}$ is presented in Fig. \ref{fig:Reso}. While the width of the distribution (Fig. \ref{fig:Reso2}) is not much influenced by the different jet energy correction stages, the mean spectrum shows a large improvement after additional correction. The raw XCone jets and the ones corrected with AK4 corrections show a dependence on $p_T$ where the corrected jets are flatly distributed. Additionally, the corrected jets show a constant mean around zero which indicates a well performing correction.
  	
  	\begin{figure}[tb]
  		\begin{subfigure}{.5\textwidth}
   		\centering
  		\includegraphics [width=\textwidth]{../Plots/Resolution_Subjets/pt_mean_rec_after}
  		\caption{}
  		\label{fig:Reso1}
  		\end{subfigure}
  		\begin{subfigure}{.5\textwidth}
   		\centering
  		\includegraphics [width=\textwidth]{../Plots/Resolution_Subjets/pt_rms_rec_after}
  		\caption{}
  		\label{fig:Reso2}
  		\end{subfigure}
  		\caption{Mean (a) and width (b) of the relative deviation in jet $p_T$ between particle and reconstruction level. The mean stays almost constant after applying the customised correction. Furthermore, the width is rather independent on the correction.}
  		\label{fig:Reso}
  	\end{figure}
  	
  	
\FloatBarrier %draw figures of previous section before the new one starts	
\subsection{Final Jet Mass Distribution}
	After the events ran through a baseline selection, a measurement phase space requirement and a new derived jet energy correction, the final jet mass distribution is presented in Fig. \ref{fig:MJet_final}. Data and Monte Carlo prediction agree very well after scaling $t\bar{t}$ simulation with a constant factor. A jet mass distribution from the $8\;\text{TeV}$ analysis is shown in Fig. \ref{fig:Torben_MJet} to compare both results. Cambridge/Aachen jets with a radius parameter of $1.2$ are used, only allowing events with one jet carrying $p_T > 500\;\text{GeV}$. A drastic improvement in peak resolution and statistics is visible in the XCone distribution, promising an improved unfolding output in this $13\;\text{TeV}$ analysis. 
	\begin{figure}[h]
  		\centering
 		\begin{tikzpicture}
 		 \node[anchor=south west,inner sep=0] (image) at (0,0)
 		 {\includegraphics[width=.8\textwidth, trim=0 0 3cm 0, clip]{../Plots/PostSel/XCone_cor_SF/M_jet1__lin.pdf}};
 		 \node[align=left,font=\small] at (3.5, 7.0) {$t\bar{t}$ scaled};
 		\end{tikzpicture} 
  		\caption{Jet mass distribution of the corrected jets clustered with XCone. This distribution will be used as input for further analysis steps.} 
  		\label{fig:MJet_final}
  	\end{figure}	
  	
	\begin{figure}[tb]
		\centering
		\includegraphics [width=.6\textwidth]{../Plots/Torben/Torben_data_paper.pdf}
		\caption{Jet mass distribution from a similar analysis at $8\;\text{TeV}$. Besides the different dataset, Cambridge/Aachen jets with a radius parameter of $1.2$ are used. In this case, only jets with $p_T > 500\;\text{GeV}$ are shown. Taken from \cite{torben_paper}.}
		\label{fig:Torben_MJet}
	\end{figure}

\section{Unfolding}
\label{sec:unfolding}
	Most analyses at LHC measure distributions of appropriate variables and then compare the obtained results in data with event simulations. In this method the MC samples also include detector effects. What one measures in this case is the real distribution on particle level folded with an unknown detector function. Studying the difference in MC between particle level and reconstruction level, it is possible to calculate the probabilities that a measured value in a bin $y_i$ is originating from bin $x_i$ on particle level. A visualisation of this problem is drawn in Fig. \ref{fig:Unfolding}.	
	\begin{figure}[tb]
		\centering
		\includegraphics [width=.6\textwidth]{../Images/Unfolding.png}
		\caption{Schematic view of an unfolding procedure. The goal is to unfold a measured distribution $\mathbf{y}$ to obtain a true distribution $\mathbf{x}$ without detector effects. Taken from \cite{tunfold}.}
		\label{fig:Unfolding}
	\end{figure}
	The matrix $\mathbf{A}$ describing migrations from bins in $\mathbf{x}$ to bins in $\mathbf{y}$ can be calculated using simulations where both distributions $\mathbf{x}$ and $\mathbf{y}$ are known. Then, the migration matrix can be used obtain an estimate for data on particle level. Following equation denotes the basic problem one has to solve:
	\begin{equation}
	\tilde{y}_i = \sum_{j=1}^{m} A_{ij}\tilde{x}_j, 1 \leq i \leq n
	\label{eq:unfold}
	\end{equation}
	where $m$ and $n$ are the number of bins of the true and measured distributions, respectively. The tilde marks the statistical mean of $\mathbf{x}$ and $\mathbf{y}$. Here, one is interested in a distribution $x_j$ and not $\tilde{x}_j$ what makes this problem not trivial. If one only replaces $\tilde{y}_i \rightarrow y_i$ and $\tilde{x}_j \rightarrow x_j$ and solve for $x_j$ by inverting the matrix $\mathbf{A}$, statistical fluctuations of $\mathbf{y}$ would be amplified. Thus, fluctuations have to be damped with a regularisation.
	
\subsection{Regularised Unfolding with TUnfold}
	The TUnfold software package \cite{tunfold} provides a framework for regularised unfolding procedures in high energy physics. Equation \ref{eq:unfold_lagrange} shows the Lagrangian implemented in TUnfold that is minimised.
	\begin{eqnarray}
	\label{eq:unfold_lagrange}
	\mathcal{L}(x,\lambda) &=& \mathcal{L}_1 + \mathcal{L}_2 + \mathcal{L}_3 
	\\ \nonumber \text{with}
	\\ 
	\label{eq:unfold_lagrange1}
	\mathcal{L}_1 &=& (\mathbf{y} - \mathbf{Ax})^\intercal \mathbf{V_{yy}}^{-1} (\mathbf{y} - \mathbf{Ax}) 
	\\
	\label{eq:unfold_lagrange2}
	\mathcal{L}_2 &=& \tau^2 (\mathbf{x} - f_b \mathbf{x}_0)^\intercal (\mathbf{L}^\intercal \mathbf{L}) (\mathbf{x} - f_b \mathbf{x}_0) 
	\\
	\label{eq:unfold_lagrange3}
	\mathcal{L}_3 &=& \lambda (Y-\mathbf{e}^\intercal \mathbf{x}) \ \text{with} \ Y=\sum_{i} y_i \ \text{and} \ e_j = \sum_{i}A_{ij}
	\end{eqnarray}
	The first term $\mathcal{L}_1$ contains a standard least square minimisation where $\mathbf{V_{yy}}$ is the covariance matrix describing uncertainties. Secondly a regularisation with strength $\tau^2$ is used. A bias vector can be introduced using a factor $f_b$ and a vector $\mathbf{x}_0$ to suppress deviations of $\mathbf{x}$ from $f_b\mathbf{x}_0$. Additionally, three choices for the matrix $\mathbf{L}$ can be made to either regularise the absolute value, first or second derivative of $\mathbf{x}$. The final term $\mathcal{L}_3$ expresses an optional area constraint, checking differences in event counts between input and output. Since the free parameter $\tau$ defines the regularisation strength, a suitable value has to be found. This is achieved by an L-Curve scan, which finds the point of largest curvature in a graph. Two variables $L_x$ and $L_y$ are defined as follows:
	\begin{eqnarray}
	L_x  &=& \log \left(  \mathcal{L}_1 \right) \\
	L_y  &=& \log \left(  \frac{\mathcal{L}_2}{\tau^2} \right)
	\end{eqnarray}
	Now, a graph is constructed in the $L_x$-$L_y$-plane for many values of $\tau$. The point of largest curvature then indicates the point, where $\mathcal{L}_1$ and $\mathcal{L}_2$ equally influence the unfolding. The value of $\tau$ then corresponds to this point. 
	
\subsection{Migration Matrix}
\label{sec:migrations}
	To perform an unfolding, a migration matrix has to be filled. A matrix entry $A_{ij}$ contains the number of events that is generated in bin $i$ and reconstructed in bin $j$. Thus, a projection on one dimension returns either the jet mass distribution on particle or reconstruction level. Only events from the $t\bar{t}$ samples are considered to construct the matrix. If an event passes the particle level selection (see section \ref{sec:GenSel}) and reconstruction level selection (see sections \ref{sec:PreSel} and \ref{sec:FinalSel}) it is filled in the according bin. To account for events that pass only one selection, underflow bins are used, while jet masses above $400\;\text{GeV}$ are filled into overflow bins. Furthermore it has to be accounted for different weights because corrections like trigger and b-tagging scale factors as well as pile-up re-weighting are only applied on reconstruction level. Thus, the weight applied to an event is split into a generator level weight and a reconstruction level weight as shown in Eq. \ref{eq:weight}.	
	\begin{equation}
	\text{event weight} = \text{gen weight} \cdot \text{reco weight}
	\label{eq:weight}
	\end{equation}
	Because events without reconstruction level information only carry a generator weight, events are filled into underflow bins with a compensation weight according to Eq. \ref{eq:weight2} to match the total number of events.
	\begin{equation}
	\text{compensation weight} = \text{gen weight} \cdot (1 - \text{reco weight})
	\label{eq:weight2}
	\end{equation}		
	Figure \ref{fig:Migration} shows the migration matrix where every bin is scaled to the total event count in the whole column, not considering underflow bins. Thus, an entry $A_{ij}$ gives the probability that an event generated in bin $i$ is reconstructed in bin $j$.
	
	\begin{figure}[tb]
		\centering
		\includegraphics [width=.6\textwidth]{../Plots/Unfolding/Data/Migration_prob.pdf}
		\caption{Display of the migration matrix. Each bin is scaled to the total event count in a column.}
		\label{fig:Migration}
	\end{figure}
	
	
\subsection{Closure Tests}
	Various tests, unfolding Monte Carlo samples are performed in this analysis. Firstly, a MC sample is unfolded with it self. Since the L-curve scan does not work when using statistically dependent samples, a $\tau$ value is set manually. Nevertheless, the output distribution of TUnfold is expected to exactly match the particle level distribution. The comparison presented in Fig \ref{fig:Unfolding_same} shows an exact accordance and proves a correct filling of the migration matrix. Another approach to furthermore test the unfolding scan is to split the MC sample into two statistically independent parts. Therefore $10\%$ are randomly selected and saved as pseudo data, while the other $90\%$ are used to construct the migration matrix. The splitting is performed four times, with every pseudo dataset containing different events. The unfolded pseudo data is compared to its distribution on particle level, referred to as truth. In all bins except the peak, unfolded distribution and truth agree very well, indicating a working setup. \\
	A further test is presented to check the unfolding for model dependence. Again, the MC sample is split, but now the renormalisation and factorisation scales are varied in the pseudo dataset. Figure \ref{fig:Unfolding_split_scale} shows the unfolded distribution of pseudo data with both scales varied up and down. Here, biases above (scaled up) and below (scaled down) the particle level distribution are visible, especially in the peak. Thus, the current unfolding setup is biased by different modelled inputs. This can be explained with the reconstruction selection efficiency. Since only about $15\%$ of events that passed the particle level selection also pass the reconstruction level selection, the unfolding is much influenced by one underflow bin of the migration matrix. As the $8\;\text{TeV}$ analysis showed  \cite{torben_paper}, this effect can be compensated by include migrations from lower $p_T$ thresholds into the matrix. For this analysis in the current status, a model uncertainty is calculated from the difference of the unfolded distribution and its truth in every bin. Here, the maximum difference of both scale variations is considered and then included into the unfolding of data.
	
	\begin{figure}[tb]
		\centering
		\includegraphics [width=.55\textwidth]{../Plots/Unfolding/MC_Same/Unfold.pdf}
		\caption{Unfolding of a MC sample with itself. An exact agreement between unfolded distribution and its truth is observed, validating a correctly filled migration matrix.}
		\label{fig:Unfolding_same}
	\end{figure}	
	

	
	\begin{figure}[tb]
		\centering
		\includegraphics [width=.55\textwidth]{../Plots/Unfolding/MC_Split_all/Unfold.pdf}
		\caption{A unfolding with a MC sample split into $10\%$ pseudo data and $90\%$ to fill the migration matrix is performed four times with different pseudo datasets. Every unfolding output is compared with a particle level distribution of the $90\%$ Monte Carlo whose uncertainty is represented by the line thickness. Fluctuations around the truth distribution is observed, probably arising from the simple migration matrix.}
		\label{fig:Unfolding_split1234}
	\end{figure}		
	
	\begin{figure}[tb]
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics [width=\textwidth]{../Plots/Unfolding/MC_Split_up/Unfold.pdf}
		\caption{}
		\label{fig:Unfolding_split_up}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics [width=\textwidth]{../Plots/Unfolding/MC_Split_down/Unfold.pdf}
		\caption{}
		\label{fig:Unfolding_split_down}
		\end{subfigure}		
		\caption{Unfolding with statistically independent MC samples where the renormalisation and factorisation scale in the pseodo dataset is scalled up (a) and down (b). Is is compared with its truth distribution where a bias is observed in both graphs.}
		\label{fig:Unfolding_split_scale}
	\end{figure}	
	
\FloatBarrier %draw figures of previous section before the new one starts	
\subsection{Unfolding of Data}
	Despite a preliminary migration matrix, a first test to unfold data is presented. Prior to the unfolding, background processes estimated from simulation are subtracted within the TUnfold software package. The unfolded distribution is depicted in comparison with the particle level distribution of the migration Monte Carlo in Fig. \ref{fig:Unfolding_data}. Alongside statistical uncertainties, indicated by the inner bars, the model uncertainty derived above is quadratically added after the unfolding, creating the outer error bars. Within the large uncertainties, the unfolded data agrees well with the MC truth. A more elaborate migration matrix is needed to damp these uncertainties which will be included in the future. In addition, a  matrix filled with statistical uncertainties of the unfolding is shown in Fig. \ref{fig:Correlations}. A diagonal structure with anti-correlated nearby bins is visible, indicating a basically well performing unfolding. 
	
	\begin{figure}[h]
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics [width=\textwidth]{../Plots/Unfolding/Data/Unfold.pdf}
		\caption{}
		\label{fig:Unfolding_data}
		\end{subfigure}		
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics [width=\textwidth]{../Plots/Unfolding/Data/Correlations.pdf}
		\caption{}
		\label{fig:Correlations}		
		\end{subfigure}	
		\caption{Unfolded data with migrations discussed in section \ref{sec:migrations} compared with the particle level distribution of the simulation used to fill the matrix is presented in (a). The inner error bars show statistical uncertainties while the outer errors include the model uncertainties derived from scale variations. The correlation matrix, containing statistical uncertainties is displayed in (b).}
	\end{figure}
	
%todo auf naechste Seite?
\section{Results}
\label{sec:results}
	After a baseline selection of a preferably pure $t\bar{t}$ sample, XCone jets are clustered to reconstruct top quark decays. The measurement phase space, selecting boosted top quarks is defined via two criteria:
	\begin{itemize}
	\item $p_T^{\text{1st jet}} > 400\;\text{GeV}$ and
	\item $M^{\text{1st jet}} > M^{\text{2nd jet + lepton}}$.
	\end{itemize}
	With this, the top quark decay can be reconstructed very well as depicted in Fig. \ref{fig:Result1}. The jet mass distribution is sensitive to the top quark mass, while obtaining an excellent resolution and high statistics. Based on this distribution, an unfolding is performed, using simulation information to fill a migration matrix. The unfolding procedure is tested and validated with pseudo data and finally applied to data. Figure \ref{fig:Result2} shows the unfolded distribution. Further improvements are expected with a more elaborate phase space in the migration matrix and a closer look into binning schemes. Nevertheless, this result shows the capability of this method and proofs a working unfolding set up.  
	
	\begin{figure}[tb]
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics [width=1.05\textwidth, trim=0 0 5cm 0, clip]{../Plots/PostSel/XCone_cor_SF/M_jet1__lin.pdf}
 		\caption{}
		\label{fig:Result1}		
		\end{subfigure}		
		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics [width=.95\textwidth,  trim=0 0.4cm 0 0, clip]{../Plots/Unfolding/Data/Unfold.pdf}
		\caption{}
		\label{fig:Result2}
		\end{subfigure}		
		\caption{As a result of this analysis, the unfolded data distribution (a) and the reconstructed jet mass (b) are shown. The very well performing XCone clustering algorithm reconstructs the top quark decay with high precision. Thus, the jet mass is sensitive to the top quark mass. The unfolded distribution agrees with the prediction by simulation but shows large model uncertainties.}
	\end{figure}
